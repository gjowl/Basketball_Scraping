@Author: Gilbert Loiseau
@Date:   2022/03/14
@Email:  gjowl04@gmail.com
@Filename: deep_learning_notes.txt
@Last modified by:   Gilbert Loiseau
@Last modified time: 2022/03/14


#Developing a neural network: notes

2022-2-8
To start, I'm watching: https://www.youtube.com/watch?v=Wo5dMEP_BbI&ab_channel=sentdex
sentdex has a whole playlist of a tutorial to build a neural network: https://www.youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3
  - This tutorial made sense to me when I started it, so I just kept going with it
  - Need prior coding experience in object oriented programing language
  - He seems to break it down in a way that makes it sound relatively simple
  - Apparently there's also a book if you want to learn from that or follow along with the videos: nnfs.io

Episode 1: Intro and Neuron Code
End goal is to take input data, pass through the neural network to get a desired output. To do this, you tune the weights and biases (training) in such a way so that the
neural network can take in new input data and still give you a reasonable output.

To do this, you end up getting a large number of tunable parameters that will impact the output of the next neuron, which could theoretically be mapped.

The important thing to build is a connection of neurons to the next neurons, parameterizing your network.

Every unique neruon has a unique bias.

First step of a neuron is to add up all of the inputs times the weights plus the bias.

Example (neuron with 3 inputs):
inputs = [1.2, 5.1, 2.1]
weights = [3.1, 2.1, 8.7]
bias = 3

output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias
print(output)

output should be 35.7

Most steps should look like the above.

Ep. 2: Coding a layer

It should be possible to randomly initialize weights that the neural network can tweak.

Example that matches the book:
inputs = [1, 2, 3]
weights = [0.2, 0.8, -0.5]
bias = 2

output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias
print(output)

output should be 2.3

Every input should have it's own unique weight, but there should only be one bias per neuron.

input layer = values that you are tracking (if basketball: points, rebounds, assists, etc.)
Adding one more input:
  - inputs can be:
    - values from the input layer
    - given values from a vector of values
  - Example (neuron with 4 inputs):
      inputs = [1, 2, 3, 2.5]
      weights = [0.2, 0.8, -0.5, 1.0]
      bias = 2

      output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] +bias
      print(output)

      output should be 4.8

What if we wanted to model 3 neurons, with 4 inputs:
  - 4 inputs
  - 3 unique weight sets/vectors
  - each weight set has 4 unique values
  - 3 total values, one for each neuron

Example (3 neuron layer):
inputs = [1, 2, 3, 2.5]

weights1 = [0.2, 0.8, -0.5, 1.0]
weights2 = [0.5, -0.91, 0.26, -0.5]
weights3 = [-0.26, -0.27, 0.17, 0.87]

bias1 = 2
bias2 = 3
bias3 = 0.5

output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] +bias1,
          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] +bias2,
          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] +bias3]
print(output)

output should be [4.8, 1.21, 2.385]

So you can't change the inputs (that's your data), but you can change your outputs by changing weights and biases.
The idea of deep learning is figuring how to adjust those weights and biases to get those outputs that you desire.

Ep. 3: The Dot Product
Transitioning to using vectors and matrices, the standard of doing deep learning.

github link: https://github.com/Sentdex/NNfSiX (has different languages that you can follow along in)

Example (3 neuron layer, but nicer):
*
inputs = [1, 2, 3, 2.5]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
*
layer_outputs = [] # output of current layer

#zip: combines two lists so that they can be easily accessed element wise (for 1: neuron_weights)
*
for neuron_weights, neuron_bias in zip(weights, biases):
    neuron_output = 0 #output of given neuron
    for n_input, weight in zip(inputs, neuron_weights):
        neuron_output += n_input*weight
    neuron_output += neuron_bias
    layer_outputs.append(neuron_object)

print(layer_outputs)
*

layer_outputs should be [4.8, 1.21, 2.385]

weights and biases are knobs that get tuned in an attempt to fit to data. These are tunable parameters
that impact a neuron's output. Weight is a multiple, and bias offsets the value. Weight is changing the
magnitude while bias is changing the ... of the output.

Next: getting into numpy
  - Shape: a list with 4

2022-3-14
Want to multiply inputs by weights by biases: Dot Product
  - dot_product = a[0]*b[0] + a[1]*b[1] + a[2]*b[2] = 20 (scalar value)

Example using dot product on small vectors:
*
import numpy as np

inputs = [1, 2, 3, 2.5]
weights = [0.2, 0.8, -0.5, 1.0]
bias = 2

output = np.dot(weights, inputs) + bias
print (output)
*
output should be 4.8

Example using dot product for larger vectors:
*
import numpy as np
inputs = [1, 2, 3, 2.5]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
output = np.dot(weights, inputs) + bias
print (output)
*
The first element you get back is the index; weights comes first (values from neurons) is the first input to index and know what the values are.
Should error out because of the shape.
What's happening:
  - np.dot(weights, inputs) = [np.dot(weights[0], inputs), np.dot(weights[1], inputs), np.dot(weights[2], inputs)] = [2.8, -1.79, 1.885]
  - Above dot product then gets added to the biases vector: [2.8+2, 3-1.79, 0.5+1.885] = [4.8, 1.21, 2.385]


Next steps:
  - passing a batch of inputs (a matrix of vectors)

Conceptually, this type of machine learning is very similar to a line:
  - y=mx+b
  - output = weight*input + bias
    - bias will adjust the crossing point of the line on the y axis
    - weight adjusts the type of function of the line

Ep. 4: Batches, Layers, and Objects
Convert a single sample into a batch, multiple layers of neurons, and conversion into objects for better pro0gramming.

Reasons for batching:
  - allows for calculations in parallel; great for GPUs allowing for so many cores to calculate on
  - helps with generalization: allow for better performance of the neural net between given data and other data
    - inputs are a single sample with x number of features. But the goal is to fit them all to a functions.
    - Basically draws a line of best fit for the number of samples per batch. Making a line of best fit is easier and faster if you have
    more samples than just one, allowing for better adjustment. This gets more important when you talk about the sample data you put in vs
    the sample data you don't. Putting all of the data in at once can create too much bias for the neural net: it will overfit to the given
    data and may not be as generalizable to other data.
  - Usually, batches are 32 or 64 but it will depend on what you are doing

Example of making the inputs a batch:
*
import numpy as np
inputs = [[1, 2, 3, 2.5],
          [2.0, 50.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
output = np.dot(weights, inputs) + bias
print (output)
*
input image of matrix dot products here: screenshot from 2022-03-14

dot product of row of matrix a by column of matrix b. Because of that, the above will give you a shape error.
The inputs shape is 3x4 and weights shape is 3x4. Need to adjust the weights matrix using transpose, swapping rows and columns.

*
import numpy as np
inputs = [[1, 2, 3, 2.5],
          [2.0, 50.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
output = np.dot(inputs, np.array(weights).T) + bias
print (output)
*
Weights array is transposed so that the number of rows and columns in inputs and weights respectively is the same, allowing for the dot product

Example of adding another layer:
*
import numpy as np
inputs = [[1, 2, 3, 2.5],
          [2.0, 50.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]

weights2 = [[0.1, 0.14, 0.5],
           [-0.5, 0.12, -0.33],
           [-0.44, -0.27, 0.17]

biases2 = [-1, 2, -0.5]
layer1_outputs = np.dot(inputs, np.array(weights).T) + biases
layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2
print (layer2_outputs)
*
Should output like the below
[[0.5031, -1.04185 -2.03875]
 [0.2434, -2/7332, -5.7633]
 [-0.99314, 1.41254, -0.35655]]

Trained models are saved weights and biases. If you load a model, you're putting those into your class.
However, if making a new one, need to start by initializing weights, somewhere within -1 to 1 (smaller values are better, limiting the size of your data).
Scaling your data is a good idea so that it keeps things within -1 and 1.

01. to 0.1 may be a good place to start.

Defining two hidden layers:
*
import numpy as np

np.random.seed(0)

X = [[1, 2, 3, 2.5],
     [2.0, 50.0, -1.0, 2.0],
     [-1.5, 2.7, 3.3, -0.8]]

class Layer_Dense:
  def_init_(self, n_inputs, n_neurons):
    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)#size of inputs and number of neurons; multiply by 0.10 makes weights between -0.1 and 0.1
    self.biases = np.zeros((1, n_neurons))
  def forward(self):
    self.output = np.dot(inputs, self.weights) + self.biases

layer1 = Layer_Dense(4, 5)
layer2 = Layer_Dense(5, 2) #input must be the size of the output of the previous layer

layer1.forward(X)
print(layer1.output)
layer2.forward(layer1.output)
print(layer2.output)
*

Ep. 5: Hidden Layer Activator Functions
Talking about rectified linear, step, and sigmoid activator functions.

Activation function:
  - comes into play after you get the output of a neuron
  - generally, the final output function has a different activation function than the hidden layers

Step function:
  - each output is either 0 or 1
  y = { 1, x>0; 0, x<=0}

Sigmoid function:
  - get a more granular output from this function
  y = 1/(1+e^-x)

Rectified linear (ReLU):
  - granular output that doesn't have a "vanishing gradient problem"
  - fast and less complicated than sigmoid calculations
    - if x > 0, output x rather than calculate x for sigmoid.
  y = {x, x>0; 0, x <=0}

After training, the important this is to be able to determine how much loss (?) you are getting throughout the run
and then training it to be more accurate. With a step function, it doesn't tell you how close you are to one or 0.
But for a sigmoid, it tells you a bit more information of how close each output is, making it a good function for
training neural networks.

ReLU is as powerful as sigmoid but faster. Can strengthen the input by increasing the weight. Then, you can offset the
activation point (moving horizontal on x axis) using bias. Negating the weight will instead determine at which point the
neuron deativates. If you add a second neuron, bias now moves it up vertically on the y axis, and negating the weight
shows at which point it activates the neuron, eventually creating a lower and upper bound, similar to sigmoid. If we're
below activation of the first neuron, the output will be the bias of the second neuron. If we're below the activation of the
second, output will be 0.

This concept gets a bit complex and the video shows it well: 9-21 minutes. The theory is quite complicated and I don't think
it's very easy to explain why and how it's able to work, but he brings up a toy example of what the neural net could be doing
just to try to make it make more sense. Each neuron has some sort of area of effect on the actual shape of the final function.
The important thing is that you need two or more hidden layers to fit non-linear functions, and using non-linear activation
functions aid in this process of fitting.

If we were just using weights and biases to train our neural net, our activation function would be linear: y = x.
Using linear activation functions, you can only really fit linear data or at best approximate using a linear function.

Example ReLU:
*
import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()#override for replication of the nnfs data and datasets.

X = [[1, 2, 3, 2.5],
     [2.0, 50.0, -1.0, 2.0],
     [-1.5, 2.7, 3.3, -0.8]]

#creates the dataset from nnfs
X, y = spiral_data(100, 3)

inputs = [0 ,2 ,-1, 3.3, -2.7, 1.1, 2.2, -100]
output = []

class Layer_Dense:
  def_init_(self, n_inputs, n_neurons):
    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)#size of inputs and number of neurons; multiply by 0.10 makes weights between -0.1 and 0.1
    self.biases = np.zeros((1, n_neurons))
  def forward(self):
    self.output = np.dot(inputs, self.weights) + self.biases

class Activation_ReLU:
  def forward(self, inputs):
    self.output = np.maximum(0.inputs)

layer1 = Layer_Dense(2, 5)
activation1 = Activation_ReLU()

layer1.forward(X)
activation1.forward(layer1.output)
print(activation1.output)
*

Look up pep8 styling for setting this up throughout multiple files instead of just having everything in one file: https://pep8.org/

Using the nnfs package:
  -pip install nnfs
  -nnfs
    -shows help options and tells how to install the code for each part of the tutorial


Ep. 6: Softmax Activation

Ep. 7: Calculating Loss with Categorical Cross-Entropy

Ep. 8: Implementing Loss

Ep. 9: Introducing Optimization and Derivatives
