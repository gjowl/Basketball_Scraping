@Author: Gilbert Loiseau
@Date:   2022/03/14
@Email:  gjowl04@gmail.com
@Filename: deep_learning_notes.txt
@Last modified by:   Gilbert Loiseau
@Last modified time: 2022/03/18


#Developing a neural network: notes

2022-2-8
To start, I'm watching: https://www.youtube.com/watch?v=Wo5dMEP_BbI&ab_channel=sentdex
sentdex has a whole playlist of a tutorial to build a neural network: https://www.youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3
  - This tutorial made sense to me when I started it, so I just kept going with it
  - Need prior coding experience in object oriented programing language
  - He seems to break it down in a way that makes it sound relatively simple
  - Apparently there's also a book if you want to learn from that or follow along with the videos: nnfs.io

Episode 1: Intro and Neuron Code
End goal is to take input data, pass through the neural network to get a desired output. To do this, you tune the weights and biases (training) in such a way so that the
neural network can take in new input data and still give you a reasonable output.

To do this, you end up getting a large number of tunable parameters that will impact the output of the next neuron, which could theoretically be mapped.

The important thing to build is a connection of neurons to the next neurons, parameterizing your network.

Every unique neuron has a unique bias.

First step of a neuron is to add up all of the inputs times the weights plus the bias.

Example (neuron with 3 inputs):
inputs = [1.2, 5.1, 2.1]
weights = [3.1, 2.1, 8.7]
bias = 3

output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias
print(output)

output should be 35.7

Most steps should look like the above.

Ep. 2: Coding a layer

It should be possible to randomly initialize weights that the neural network can tweak.

Example that matches the book:
inputs = [1, 2, 3]
weights = [0.2, 0.8, -0.5]
bias = 2

output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias
print(output)

output should be 2.3

Every input should have it's own unique weight, but there should only be one bias per neuron.

input layer = values that you are tracking (if basketball: points, rebounds, assists, etc.)
Adding one more input:
  - inputs can be:
    - values from the input layer
    - given values from a vector of values
  - Example (neuron with 4 inputs):
      inputs = [1, 2, 3, 2.5]
      weights = [0.2, 0.8, -0.5, 1.0]
      bias = 2

      output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + inputs[3]*weights[3] +bias
      print(output)

      output should be 4.8

What if we wanted to model 3 neurons, with 4 inputs:
  - 4 inputs
  - 3 unique weight sets/vectors
  - each weight set has 4 unique values
  - 3 total values, one for each neuron

Example (3 neuron layer):
inputs = [1, 2, 3, 2.5]

weights1 = [0.2, 0.8, -0.5, 1.0]
weights2 = [0.5, -0.91, 0.26, -0.5]
weights3 = [-0.26, -0.27, 0.17, 0.87]

bias1 = 2
bias2 = 3
bias3 = 0.5

output = [inputs[0]*weights1[0] + inputs[1]*weights1[1] + inputs[2]*weights1[2] + inputs[3]*weights1[3] +bias1,
          inputs[0]*weights2[0] + inputs[1]*weights2[1] + inputs[2]*weights2[2] + inputs[3]*weights2[3] +bias2,
          inputs[0]*weights3[0] + inputs[1]*weights3[1] + inputs[2]*weights3[2] + inputs[3]*weights3[3] +bias3]
print(output)

output should be [4.8, 1.21, 2.385]

So you can't change the inputs (that's your data), but you can change your outputs by changing weights and biases.
The idea of deep learning is figuring how to adjust those weights and biases to get those outputs that you desire.

Ep. 3: The Dot Product
Transitioning to using vectors and matrices, the standard of doing deep learning.

github link: https://github.com/Sentdex/NNfSiX (has different languages that you can follow along in)

Example (3 neuron layer, but nicer):
*
inputs = [1, 2, 3, 2.5]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
*
layer_outputs = [] # output of current layer

#zip: combines two lists so that they can be easily accessed element wise (for 1: neuron_weights)
*
for neuron_weights, neuron_bias in zip(weights, biases):
    neuron_output = 0 #output of given neuron
    for n_input, weight in zip(inputs, neuron_weights):
        neuron_output += n_input*weight
    neuron_output += neuron_bias
    layer_outputs.append(neuron_object)

print(layer_outputs)
*

layer_outputs should be [4.8, 1.21, 2.385]

weights and biases are knobs that get tuned in an attempt to fit to data. These are tunable parameters
that impact a neuron's output. Weight is a multiple, and bias offsets the value. Weight is changing the
magnitude while bias is changing the ... of the output.

Next: getting into numpy
  - Shape: a list with 4

2022-3-14
Want to multiply inputs by weights by biases: Dot Product
  - dot_product = a[0]*b[0] + a[1]*b[1] + a[2]*b[2] = 20 (scalar value)

Example using dot product on small vectors:
*
import numpy as np

inputs = [1, 2, 3, 2.5]
weights = [0.2, 0.8, -0.5, 1.0]
bias = 2

output = np.dot(weights, inputs) + bias
print (output)
*
output should be 4.8

Example using dot product for larger vectors:
*
import numpy as np
inputs = [1, 2, 3, 2.5]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
output = np.dot(weights, inputs) + bias
print (output)
*
The first element you get back is the index; weights comes first (values from neurons) is the first input to index and know what the values are.
Should error out because of the shape.
What's happening:
  - np.dot(weights, inputs) = [np.dot(weights[0], inputs), np.dot(weights[1], inputs), np.dot(weights[2], inputs)] = [2.8, -1.79, 1.885]
  - Above dot product then gets added to the biases vector: [2.8+2, 3-1.79, 0.5+1.885] = [4.8, 1.21, 2.385]


Next steps:
  - passing a batch of inputs (a matrix of vectors)

Conceptually, this type of machine learning is very similar to a line:
  - y=mx+b
  - output = weight*input + bias
    - bias will adjust the crossing point of the line on the y axis
    - weight adjusts the type of function of the line

Ep. 4: Batches, Layers, and Objects
Convert a single sample into a batch, multiple layers of neurons, and conversion into objects for better pro0gramming.

Reasons for batching:
  - allows for calculations in parallel; great for GPUs allowing for so many cores to calculate on
  - helps with generalization: allow for better performance of the neural net between given data and other data
    - inputs are a single sample with x number of features. But the goal is to fit them all to a functions.
    - Basically draws a line of best fit for the number of samples per batch. Making a line of best fit is easier and faster if you have
    more samples than just one, allowing for better adjustment. This gets more important when you talk about the sample data you put in vs
    the sample data you don't. Putting all of the data in at once can create too much bias for the neural net: it will overfit to the given
    data and may not be as generalizable to other data.
  - Usually, batches are 32 or 64 but it will depend on what you are doing

Example of making the inputs a batch:
*
import numpy as np
inputs = [[1, 2, 3, 2.5],
          [2.0, 50.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
output = np.dot(weights, inputs) + bias
print (output)
*
input image of matrix dot products here: screenshot from 2022-03-14

dot product of row of matrix a by column of matrix b. Because of that, the above will give you a shape error.
The inputs shape is 3x4 and weights shape is 3x4. Need to adjust the weights matrix using transpose, swapping rows and columns.

*
import numpy as np
inputs = [[1, 2, 3, 2.5],
          [2.0, 50.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]
output = np.dot(inputs, np.array(weights).T) + bias
print (output)
*
Weights array is transposed so that the number of rows and columns in inputs and weights respectively is the same, allowing for the dot product

Example of adding another layer:
*
import numpy as np
inputs = [[1, 2, 3, 2.5],
          [2.0, 50.0, -1.0, 2.0],
          [-1.5, 2.7, 3.3, -0.8]]

weights = [[0.2, 0.8, -0.5, 1.0],
           [0.5, -0.91, 0.26, -0.5],
           [-0.26, -0.27, 0.17, 0.87]]

biases = [2, 3, 0.5]

weights2 = [[0.1, 0.14, 0.5],
           [-0.5, 0.12, -0.33],
           [-0.44, -0.27, 0.17]

biases2 = [-1, 2, -0.5]
layer1_outputs = np.dot(inputs, np.array(weights).T) + biases
layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2
print (layer2_outputs)
*
Should output like the below
[[0.5031, -1.04185 -2.03875]
 [0.2434, -2/7332, -5.7633]
 [-0.99314, 1.41254, -0.35655]]

Trained models are saved weights and biases. If you load a model, you're putting those into your class.
However, if making a new one, need to start by initializing weights, somewhere within -1 to 1 (smaller values are better, limiting the size of your data).
Scaling your data is a good idea so that it keeps things within -1 and 1.

01. to 0.1 may be a good place to start.

Defining two hidden layers:
*
import numpy as np

np.random.seed(0)

X = [[1, 2, 3, 2.5],
     [2.0, 50.0, -1.0, 2.0],
     [-1.5, 2.7, 3.3, -0.8]]

class Layer_Dense:
  def_init_(self, n_inputs, n_neurons):
    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)#size of inputs and number of neurons; multiply by 0.10 makes weights between -0.1 and 0.1
    self.biases = np.zeros((1, n_neurons))
  def forward(self):
    self.output = np.dot(inputs, self.weights) + self.biases

layer1 = Layer_Dense(4, 5)
layer2 = Layer_Dense(5, 2) #input must be the size of the output of the previous layer

layer1.forward(X)
print(layer1.output)
layer2.forward(layer1.output)
print(layer2.output)
*

Ep. 5: Hidden Layer Activator Functions
Talking about rectified linear, step, and sigmoid activator functions.

Activation function:
  - comes into play after you get the output of a neuron
  - generally, the final output function has a different activation function than the hidden layers

Step function:
  - each output is either 0 or 1
  y = { 1, x>0; 0, x<=0}

Sigmoid function:
  - get a more granular output from this function
  y = 1/(1+e^-x)

Rectified linear (ReLU):
  - granular output that doesn't have a "vanishing gradient problem"
  - fast and less complicated than sigmoid calculations
    - if x > 0, output x rather than calculate x for sigmoid.
  y = {x, x>0; 0, x <=0}

After training, the important this is to be able to determine how much loss (?) you are getting throughout the run
and then training it to be more accurate. With a step function, it doesn't tell you how close you are to one or 0.
But for a sigmoid, it tells you a bit more information of how close each output is, making it a good function for
training neural networks.

ReLU is as powerful as sigmoid but faster. Can strengthen the input by increasing the weight. Then, you can offset the
activation point (moving horizontal on x axis) using bias. Negating the weight will instead determine at which point the
neuron deativates. If you add a second neuron, bias now moves it up vertically on the y axis, and negating the weight
shows at which point it activates the neuron, eventually creating a lower and upper bound, similar to sigmoid. If we're
below activation of the first neuron, the output will be the bias of the second neuron. If we're below the activation of the
second, output will be 0.

This concept gets a bit complex and the video shows it well: 9-21 minutes. The theory is quite complicated and I don't think
it's very easy to explain why and how it's able to work, but he brings up a toy example of what the neural net could be doing
just to try to make it make more sense. Each neuron has some sort of area of effect on the actual shape of the final function.
The important thing is that you need two or more hidden layers to fit non-linear functions, and using non-linear activation
functions aid in this process of fitting.

If we were just using weights and biases to train our neural net, our activation function would be linear: y = x.
Using linear activation functions, you can only really fit linear data or at best approximate using a linear function.

Example ReLU:
*
import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()#override for replication of the nnfs data and datasets.

X = [[1, 2, 3, 2.5],
     [2.0, 50.0, -1.0, 2.0],
     [-1.5, 2.7, 3.3, -0.8]]

#creates the dataset from nnfs
X, y = spiral_data(100, 3)

inputs = [0 ,2 ,-1, 3.3, -2.7, 1.1, 2.2, -100]
output = []

class Layer_Dense:
  def_init_(self, n_inputs, n_neurons):
    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)#size of inputs and number of neurons; multiply by 0.10 makes weights between -0.1 and 0.1
    self.biases = np.zeros((1, n_neurons))
  def forward(self):
    self.output = np.dot(inputs, self.weights) + self.biases

class Activation_ReLU:
  def forward(self, inputs):
    self.output = np.maximum(0.inputs)

layer1 = Layer_Dense(2, 5)
activation1 = Activation_ReLU()

layer1.forward(X)
activation1.forward(layer1.output)
print(activation1.output)
*

Look up pep8 styling for setting this up throughout multiple files instead of just having everything in one file: https://pep8.org/

Using the nnfs package:
  -pip install nnfs
  -nnfs
    -shows help options and tells how to install the code for each part of the tutorial

2022-3-15
Ep. 6: Softmax Activation
The first part in training a model is determining how wrong the model is.
Not many good solid ways of determining the difference in how wrong neurons are,
but softmax activation helps us solve that.

Ideally, output values would be a probability distribution and you can overall calculate
how accurate something is.

If you're using ReLU, if the value of an output is negative, it will give a 0. This makes
it impossible to learn from these values.

Exponential function: y=e^x. Solves the issue of having negative outputs without tossing away
the meaning of a negative value: it keeps it on a scale.

Example of softmax activation exponentiation:
*
import math

layer_outputs = [4.8, 1.21, 2.385]

E = math.e

#rids of any negative values
for output in layer_outputs:
  exp_values.append(E**output)

print(exp_values)

norm_base = sum(exp_values)
norm_values = []

for value in exp_values:
  norm_values.append(value / norm_base)

print(norm_values)
#should add up to 1
print(sum(norm_values))
*

#shorter version of the above using numpy
*
import numpy as np

layer_outputs = [4.8, 1.21, 2.385]

#rids of any negative values by exponentiating (y=e^x)
exp_values = np.exp(layer_outputs)
norm_values = exp_values / np.sum(exp_values)

print(norm_values)
#should add up to 1
print(sum(norm_values))
*

#conversion of the above to a batch
*
import numpy as np

layer_outputs = [[4.8, 1.21, 2.385],
                 [8.9, -1.81, 0.2],
                 [1.41, 1.051, 0.026]]

exp_values = np.exp(layer_outputs)

#axis=0 is the sum of columns, axis=1 is sum of rows
#The above does not have the proper shape, so keepdims=True
#makes it the proper dimensions
norm_values = exp_values / np.sum(exp_values, axis=1, keepdims=True)
*

Overflow prevention: subtract the largest value in a layer by all of the other values in that layer.
This leads to the range of possibilities post exponentiation is from 0 to 1.

Adding softmax activation class to code:
*
import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()#override for replication of the nnfs data and datasets.

class Layer_Dense:
  def_init_(self, n_inputs, n_neurons):
    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)#size of inputs and number of neurons; multiply by 0.10 makes weights between -0.1 and 0.1
    self.biases = np.zeros((1, n_neurons))
  def forward(self):
    self.output = np.dot(inputs, self.weights) + self.biases

class Activation_ReLU:
  def forward(self, inputs):
    self.output = np.maximum(0.inputs)

class Activation_Softmax:
  def forward(self, inputs):
    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))#subtracts the max of a layer (row) and keeps dimensions
    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)
    self.output = probabilities

X,y = spiral_data(samples=100, classes=3)

dense1 = Layer_Dense(2,3)
activation1 = Activation_ReLU()
dense2 = Layer_Dense(3, 3)#output layer outputs 3 for the 3 classes of data above
activation2 = Activation_Softmax()

dense1.forward(X)
activation1.forward(dense1.output)

dense2.forward(activation1.output)
activation2.forward(dense2.output)
*

To calculate how right vs how wrong each neuron in each layer may be, we need a loss function.

Ep. 7: Calculating Loss with Categorical Cross-Entropy
Important to have a metric for error: how wrong is the model that we're trying to train.

Why not use accuracy?
  -optimize neural network by binary true or false
    -imagine going through images of cats vs dogs and it only outputs yes or no for cat
    -instead, seeing how confident the model is tells you more information: 55% likely to be a cat and 45% dog

From our distribution, we can know from the training data what the target value is (?)
In general, the loss function for classification using softmax is Categorical Cross-Entropy: image saved
as screenshot 2022-3-15. 3:30 mark in the video. Simplifies to another screenshot because of one-hot encoding
This method is very convenient in the back propogation and optimization steps.

What is one-hot encoding:
  -vector that is n classes long filled with 0 except for at the index of the target class, where it will be 1
  -Examples:
    -Classes: 3; Label: 2; One-hot: [0, 0, 1]
    -Classes: 4; Label: 1; One-hot: [0, 1, 0, 0]

Categorical cross entropy: take the negative sum (-sum) of the target value (yi) times the log of the predicted value (yhatj)

*
import math

softmax_output = [0.7, 0.1, 0.2]
target_output = [1, 0, 0]

loss = -(math.log(softmax_output[0])*target_output[0] +
         math.log(softmax_output[1])*target_output[1] +
         math.log(softmax_output[2])*target_output[2])

print(loss)

#above becomes the same as below since targets are 0 at non zero position
loss = -math.log(softmax_output[0])
print(loss)

print(-math.log(0.5))#this value is higher because it is more loss, therefore more error

I don't yet know where the target_output comes from, but I hope he explains in the next video.

Ep. 8: Implementing Loss
Applying loss and addressing hurdles that could occur.

Need to apply a batch of data and a batch of targets for the softmax outputs.
To get confidences, get the value of the corresponding output for each target.

Example on getting confidence values:
*
import numpy as np

softmax_outputs = np.array([[0.7, 0.1, 0.2],
                            [0.1, 0.5, 0.4],
                            [0.02, 0.9, 0.08]])

class_targets = [0, 1, 1]

print(softmax_outputs[[0, 1, 2], class_targets])
#The above iterates through the entire np array [0, 1, 2] and gets the corresponding targets [0, 1, 1]
*

Simplifying with numpy:
*
import numpy as np

softmax_outputs = np.array([[0.7, 0.1, 0.2],
                            [0.1, 0.5, 0.4],
                            [0.02, 0.9, 0.08]])

class_targets = [0, 1, 1]

neg_log = -np.logsoftmax_outputs[range(len(softmax_outputs)), class_targets])
# the above however gives problems because if you get a 0, -log(0) is infinite,
# which isn't the best for future calculations
*

Predicting accuracy:
*
import numpy as np

softmax_outputs = np.array([[0.7, 0.1, 0.2],
                            [0.1, 0.5, 0.4],
                            [0.02, 0.9, 0.08]])

class_targets = [0, 1, 1]

predictions = np.argmax(softmax_outputs, axis=1)

accuracy = np.mean(predictions == class_targets)#get mean: Check how often the target matched the predicted output

accuracy is the mean of the predictions and the class targets
neg_log = -np.logsoftmax_outputs[range(len(softmax_outputs)), class_targets])
# the above however gives problems because if you get a 0, -log(0) is infinite,
# which isn't the best for future calculations

Adding loss class to code:
*
import numpy as np
import nnfs
from nnfs.datasets import spiral_data

nnfs.init()#override for replication of the nnfs data and datasets.

class Layer_Dense:
  def_init_(self, n_inputs, n_neurons):
    self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)#size of inputs and number of neurons; multiply by 0.10 makes weights between -0.1 and 0.1
    self.biases = np.zeros((1, n_neurons))
  def forward(self):
    self.output = np.dot(inputs, self.weights) + self.biases

class Activation_ReLU:
  def forward(self, inputs):
    self.output = np.maximum(0.inputs)

class Activation_Softmax:
  def forward(self, inputs):
    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))#subtracts the max of a layer (row) and keeps dimensions
    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)
    self.output = probabilities

class Loss:
  def calculate(self, output, y):
    sample_losses = self.forward(output, y)
    data_loss = np.mean(sample_losses)
    return data_loss

#Inherits from the base Loss class
class Loss_CategorialCrossEntropy(Loss):
  def forward(self, y_pred, y_true):
    samples = len(y_pred)
    #the below prevents values from going to infinity, clipping at 1e-7
    y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)

    #below means that scalar values have been passed ([0,1,2],[0,1,1])
    if len(y_true.shape) == 1:
      correct_confidences = y_pred_clipped[range(samples), y_true]
    #below is for one hot encoded vectors ([[1,0,0],[0,1,0],[0,1,0]]); multiplies each in predicted neurons
    #against the target classes confidence; since things are multiplied by 0 outside of the target,
    #sum gets you the output confidence vector
    elif len(y_true.shape) == 2:
      correct_confidences = np.sum(y_pred_clipped*y_true, axis=1)
    negative_log_likelihoods = -np.log(correct_confidences)
    return negative_log_likelihoods
X,y = spiral_data(samples=100, classes=3)

dense1 = Layer_Dense(2,3)
activation1 = Activation_ReLU()
dense2 = Layer_Dense(3, 3)#output layer outputs 3 for the 3 classes of data above
activation2 = Activation_Softmax()

dense1.forward(X)
activation1.forward(dense1.output)

dense2.forward(activation1.output)
activation2.forward(dense2.output)

loss_function = Loss_CategorialCrossEntropy()
loss = loss_function.calculate(activation2.output, y)

print("Loss: ", loss)
*

Ep. 9: Introducing Optimization and Derivatives
#Introduce optimization and the calculus to overcome this challenge
"""
Solving using weights and biases is a way to improve the model, but the
best way to do this would be to decrease the loss. Randomly hunting for
weight and bias combinations and looking for loss is a way to do it, but 
it doesn't decrease the loss very well or very quickly.

Instead making random tweaks to the current best weights and biases may
work better for optimizing. It works for datasets that are pretty easy to
solve, but not for more complex models. An issue is that we're treating each
of these weights and biases as equal, whereas it's likely that some weights
or biases may be more impactful.

To address this, derivatives are useful. Remember, derivatives are the change
in x over change in y, or deltax/deltay. This can be used to measure the impact
of x over y by using the slope of the tangent on the curve, or the derivative of
the function. It's important to know the impact of each value of the parameters
on the output of the function.

It's not that simple in a neural network, because we have so many nodes and
parameters in a neural network. Attempting to adjust these using numerical differentiation
is quite difficult: would need to do a pass through the network, calculate loss, change
1 parameter, pass through network, calculate loss, reset and switch to another parameter,
and continue iteratively. And we'd need to do that for each input sample. Using 
numerical derivatives, it would be like brute forcing the issue, which is slower than ideal.

Analytical and partial derivatives would be the better option.
"""